{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Graph_Collaborative_Filtering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mauricioyc/ngcf_pytorch_g61/blob/master/Neural_Graph_Collaborative_Filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5CB-1PeJL0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f59a75a-7f26-4029-fd25-47af3aa6c833"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwFpIZxgKeGN"
      },
      "source": [
        "import pandas as pd\r\n",
        "from time import time\r\n",
        "from datetime import datetime\r\n",
        "import torch\r\n",
        "import os\r\n",
        "import sys\r\n",
        "\r\n",
        "# drive path to the project\r\n",
        "source_path = \"drive/MyDrive/Machine Learning/git/ngcf_pytorch_g61\"\r\n",
        "sys.path.insert(0,source_path)\r\n",
        "\r\n",
        "# torch cuda initialization\r\n",
        "use_cuda = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAkDidOXX_C2"
      },
      "source": [
        "# Introduction\r\n",
        "\r\n",
        "Neural Graph Collaborative Filtering (NGCF), created by [Wang et al.(2019)] (https://arxiv.org/abs/1905.08108), is a Deep Learning Recommendation algorithm with graph topology that creates user-item embeddings representation and interactions. In this article we will use [MovieLens: ML-100k dataset](https://grouplens.org/datasets/movielens/100k/) to explore [this](https://github.com/metahexane/ngcf_pytorch_g61) PyTorch implementation of the NGCF.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMiynDZ4elQI"
      },
      "source": [
        "# Background Information\r\n",
        "\r\n",
        "![imagem](https://drive.google.com/uc?export=view&id=1rAQFHvKvVieAI7pVmfYZPTrHuPxn16n2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlycPzP6g8Xo"
      },
      "source": [
        "# NGCF\r\n",
        "\r\n",
        "In a traditional collaborative filtering algorithm for ML-100k, the user-item interaction can be represented by a sparse matrix $R_{n,m}$ of liked/watched movies, where $n$ is the number of users and $m$ the movies. This matrix can be factored into two feature matrices in the form\r\n",
        "$R_{n,m} = U_{n,f} \\times I_{f,m}$, where $f$ is the feature vector size as shown in the figure 2. We can call these factors **Embeddings**.\r\n",
        "\r\n",
        "<br>\r\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1uBb6pFf12tU6l29YPMBb_2YCX-T4SBbY\" height=\"300\"><figcaption>Figure 2: Matrix factorization of $R_{n,m} = U_{n,f} \\times I_{f,m}$.</figcaption></center>\r\n",
        "</br>\r\n",
        "\r\n",
        "In the NGCF, the user-item relationship can be interpreted as a bipartite graph, shown in figure 3, in which node $u_i$ and $i_i$ is a unique user and movie embedding, respectively, and the edge represents the movies each user has seen. The graph can be created from the sparce matrix $R_{n,m}$.\r\n",
        "\r\n",
        "<br>\r\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1GO5fqKaC_gRnijLjjff1nvtv4X--wMIw\" ><figcaption>Figure 3: User-item bipartite graph.</figcaption></center>\r\n",
        "</br>\r\n",
        "\r\n",
        "In the paper, the NGCF propagates the messages from the user-item embeddings relationship over the the bipartate graph, being able to capture neighbors information depending on the connectivity order parameterized. For example, a 2nd-oder connectivity can capture the path $u1 \\leftarrow i2 \\leftarrow u2$ and a 3rd-order connectivity send information to $u1$ through the path $u1 \\leftarrow i2 \\leftarrow u2 \\leftarrow i3$, being able to capture the fact that user 1 might like movie 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkgG9okp6qfR"
      },
      "source": [
        "## 1: NGCF Architecture and Message Propagation\r\n",
        "\r\n",
        "The NGCF is composed by an user and item Embeddings Layer, followed by an Embedding Propagation Layer, that receives the embeddings and calculates an recurrent update of these embeddings, and a final Prediction Layer that calculates the final prediction for a item-user pair. The layers are explained in the following.\r\n",
        "\r\n",
        "<br>\r\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1C4XBYjHCQAcNYFpyT73gvsjoWWWjabaS\" height=\"400\"><figcaption>Figure 4: NGCF architecture.</figcaption></center>\r\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RSi3k6e0C-K"
      },
      "source": [
        "### 1.1: Embedding Layer\r\n",
        "\r\n",
        "The initial user and item embeddings are concatenated in an embedding lookup table as shown in the equation below. This embedding table is initialized using the user and item embeddings with a xavier uniform and will be optimized in an end-to-end fashion by the network.\r\n",
        "\r\n",
        "<br>\r\n",
        "<center>$E = [user\\_embeddings \\space , \\space item\\_embeddings] = [e_{u_1},...,e_{u_n} \\space , \\space e_{i_1},...,e_{i_m}]$ </center>\r\n",
        "</br>\r\n",
        "\r\n",
        "```python\r\n",
        "# initialize weights\r\n",
        "def _init_weights(self):\r\n",
        "    print(\"Initializing weights...\")\r\n",
        "    weight_dict = nn.ParameterDict()\r\n",
        "\r\n",
        "    initializer = torch.nn.init.xavier_uniform_\r\n",
        "    \r\n",
        "    weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\r\n",
        "    weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\r\n",
        "\r\n",
        "# ... omitted code\r\n",
        "\r\n",
        "# creating E\r\n",
        "def forward(self, u, i, j):\r\n",
        "  # ... omitted code\r\n",
        "  ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\r\n",
        "  # ... omitted code\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsGlfgGa6M5h"
      },
      "source": [
        "### 1.2: Embedding Propagation Layers\r\n",
        "\r\n",
        "The Embedding Propagation Layers receives the user-item embbedings, as well as the graph connectivety to construct the message. The message takes into consideration the current embedding state of $e_u^{0}$ and $e_i^{0}$ and updates in a recurrent form the respective embedding. In each step $l$, a $e_u^{l}$ and $e_i^{l}$ is created and concatenated to form the final features to the Predict Layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6VqDDx3W5GK"
      },
      "source": [
        "#### 1.2.2: Message Definition:\r\n",
        "\r\n",
        "<center>\r\n",
        "$\\begin{equation}\r\n",
        "  \\begin{cases}\r\n",
        "      m^{(l)}_{u \\space \\leftarrow \\space i}  = \\frac{1}{\\sqrt{|\\mathcal{N}_u||\\mathcal{N}_i|}}\\left(W^{(l)}_1e^{(l-1)}_i + W^{(l)}_2(e^{(l-1)}_i \\space \\odot \\space e^{(l-1)}_u)\\right)  \\\\\r\n",
        "      m^{(l)}_{u \\space \\leftarrow \\space u}  = W^{(l)}_1e^{(l-1)}_u \\\\\r\n",
        "    \\end{cases}       \r\n",
        "\\end{equation}$\r\n",
        "<br></br>\r\n",
        "</center>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LPmgdjmW7iF"
      },
      "source": [
        "#### 1.2.1: Message Aggregation\r\n",
        "\r\n",
        "<center>\r\n",
        "$e^{(l)}_{u} = \\text{LeakyReLU}(m^{(l)}_{u \\space \\leftarrow \\space u} + \\sum_{i \\space \\in \\space \\mathcal{N}_u} m^{(l)}_{u \\space \\leftarrow \\space i})$\r\n",
        "</center>\r\n",
        "<br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22r49MIyW-c9"
      },
      "source": [
        "#### 1.2.3: Matrix Form:\r\n",
        "\r\n",
        "<center>\r\n",
        "$E^{(l)} = \\text{LeakyReLU}\\left((\\mathcal{L} + I)E^{(l-1)}W^{(l)}_1 + \\mathcal{L}E^{(l-1)} \\space \\odot \\space E^{(l-1)}W^{(l)}_2\\right)$\r\n",
        "</center>\r\n",
        "\r\n",
        "where $\\mathcal{L}$ represents the Laplacian for the user-item graph, which is formulated as:\r\n",
        " \r\n",
        "<center>\r\n",
        "$\\mathcal{L} = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$ and \r\n",
        "$A = \\begin{equation}\r\n",
        "  \\begin{bmatrix}\r\n",
        "      0&R \\\\\r\n",
        "      R^T&0 \r\n",
        "    \\end{bmatrix}       \r\n",
        "\\end{equation}$\r\n",
        "</center>\r\n",
        "\r\n",
        "In the code, first we inicialize the $A$ and $\\mathcal{L}$ from the train set. \r\n",
        "\r\n",
        "```python\r\n",
        "# Creating A = (L + I), where R = Train_Dok_matrix\r\n",
        "adj_mat[:self.n_users, self.n_users:] = R\r\n",
        "adj_mat[self.n_users:, :self.n_users] = R.T\r\n",
        "\r\n",
        "# normalize adjacency matrix\r\n",
        "def normalized_adj_single(adj):\r\n",
        "    rowsum = np.array(adj.sum(1))\r\n",
        "\r\n",
        "    d_inv = np.power(rowsum, -.5).flatten()\r\n",
        "    d_inv[np.isinf(d_inv)] = 0.\r\n",
        "    d_mat_inv = sp.diags(d_inv)\r\n",
        "\r\n",
        "    norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\r\n",
        "    return norm_adj.tocoo()\r\n",
        "adj_mtx = normalized_adj_single(adj_mat) + sp.eye(adj_mat.shape[0])\r\n",
        "\r\n",
        "# from A, create L = (A - I)\r\n",
        "self.adj_mtx = adj_mtx\r\n",
        "self.laplacian = adj_mtx - sp.eye(adj_mtx.shape[0])\r\n",
        "\r\n",
        "# Create Matrix 'A', PyTorch sparse tensor of SP adjacency_mtx\r\n",
        "self.A = self._convert_sp_mat_to_sp_tensor(self.adj_mtx)\r\n",
        "self.L = self._convert_sp_mat_to_sp_tensor(self.laplacian)\r\n",
        "\r\n",
        "# apply drop-out mask\r\n",
        "A_hat = self._droupout_sparse(self.A) if self.node_dropout > 0 else self.A\r\n",
        "L_hat = self._droupout_sparse(self.L) if self.node_dropout > 0 else self.L\r\n",
        "```\r\n",
        "\r\n",
        "Then, using the first embedding layer created $e^0$, a recurrent propagation calculates the following embeddings with message information $e^l_u$ and $e^l_i$, where:\r\n",
        "\r\n",
        "<center>\r\n",
        "$\\text{side_embeddings} = (\\mathcal{L} + I)E^{(l-1)}W^{(l)}_1$\r\n",
        "\r\n",
        "$\\text{side_L_embeddings} = \\mathcal{L}E^{(l-1)} \\space \\odot \\space E^{(l-1)}W^{(l)}_2$\r\n",
        "</center>\r\n",
        "\r\n",
        "```python\r\n",
        "# forward pass for 'n' propagation layers\r\n",
        "for k in range(self.n_layers):\r\n",
        "    # weighted sum messages of neighbours\r\n",
        "    side_embeddings = torch.sparse.mm(A_hat, ego_embeddings)\r\n",
        "    side_L_embeddings = torch.sparse.mm(L_hat, ego_embeddings)\r\n",
        "\r\n",
        "    # transformed sum weighted sum messages of neighbours\r\n",
        "    sum_embeddings = torch.matmul(side_embeddings, self.weight_dict['W_gc_%d' % k]) + self.weight_dict['b_gc_%d' % k]\r\n",
        "\r\n",
        "    # bi messages of neighbours\r\n",
        "    bi_embeddings = torch.mul(ego_embeddings, side_L_embeddings)\r\n",
        "    # transformed bi messages of neighbours\r\n",
        "    bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_bi_%d' % k]) + self.weight_dict['b_bi_%d' % k]\r\n",
        "\r\n",
        "    # non-linear activation \r\n",
        "    ego_embeddings = F.leaky_relu(sum_embeddings + bi_embeddings)\r\n",
        "    # + message dropout\r\n",
        "    mess_dropout_mask = nn.Dropout(self.mess_dropout)\r\n",
        "    ego_embeddings = mess_dropout_mask(ego_embeddings)\r\n",
        "\r\n",
        "    # normalize activation\r\n",
        "    norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\r\n",
        "\r\n",
        "    all_embeddings.append(norm_embeddings)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V9MRkeEPGaw"
      },
      "source": [
        "### 1.3: Prediction Layer\r\n",
        "\r\n",
        "The model prediction calculates the multiplication of $e^*_u$ by $e^*_i$, which is the logits of the user $u$ to see the movie $i$. \r\n",
        "\r\n",
        "<center>\r\n",
        "$\\hat{y}_NGCF = e^{* \\top}_u e^*_i, \\space$ where $\\space e^*_u = e^{(0)}_u || \\space .... \\space || e^{(L)}_u \\space \\text{and} \\space \\space e^*_i = e^{(0)}_i || \\space .... \\space || e^{(L)}_i$\r\n",
        "</center>\r\n",
        "\r\n",
        "```python\r\n",
        "all_embeddings = torch.cat(all_embeddings, 1)\r\n",
        "\r\n",
        "# back to user/item dimension\r\n",
        "u_g_embeddings, i_g_embeddings = all_embeddings.split([self.n_users, self.n_items], 0)\r\n",
        "\r\n",
        "# back to user/item dimension\r\n",
        "u_g_embeddings, i_g_embeddings = all_embeddings.split([self.n_users, self.n_items], 0)\r\n",
        "\r\n",
        "self.u_g_embeddings = nn.Parameter(u_g_embeddings)\r\n",
        "self.i_g_embeddings = nn.Parameter(i_g_embeddings)\r\n",
        "\r\n",
        "u_emb = u_g_embeddings[u] # user embeddings\r\n",
        "p_emb = i_g_embeddings[i] # positive item embeddings\r\n",
        "n_emb = i_g_embeddings[j] # negative item embeddings\r\n",
        "\r\n",
        "y_ui = torch.mul(u_emb, p_emb).sum(dim=1)\r\n",
        "y_uj = torch.mul(u_emb, n_emb).sum(dim=1)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrF4XQnSXCw4"
      },
      "source": [
        "### 1.4: BRP Loss\r\n",
        "\r\n",
        "```python\r\n",
        "log_prob = (torch.log(torch.sigmoid(y_ui-y_uj))).mean()\r\n",
        "\r\n",
        "# compute bpr-loss\r\n",
        "bpr_loss = -log_prob\r\n",
        "if self.reg > 0.:\r\n",
        "    l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\r\n",
        "    l2reg  = self.reg*l2norm\r\n",
        "    bpr_loss =  -log_prob + l2reg\r\n",
        "\r\n",
        "return bpr_loss\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTKa_gnZViNI"
      },
      "source": [
        "## 2: Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drhwG2qmXF0D"
      },
      "source": [
        "\r\n",
        "### 2.1: Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuLoZxlw21mG"
      },
      "source": [
        "\r\n",
        "import random as rd\r\n",
        "import scipy.sparse as sp\r\n",
        "import numpy as np\r\n",
        "[]\r\n",
        "class Data(object):\r\n",
        "    def __init__(self, path, batch_size):\r\n",
        "        self.path = path\r\n",
        "        self.batch_size = batch_size\r\n",
        "\r\n",
        "        train_file = path + '/train.txt'\r\n",
        "        test_file = path + '/test.txt'\r\n",
        "\r\n",
        "        #get number of users and items\r\n",
        "        self.n_users, self.n_items = 0, 0\r\n",
        "        self.n_train, self.n_test = 0, 0\r\n",
        "        self.neg_pools = {}\r\n",
        "\r\n",
        "        self.exist_users = []\r\n",
        "\r\n",
        "        # search train_file for max user_id/item_id\r\n",
        "        with open(train_file) as f:\r\n",
        "            for l in f.readlines():\r\n",
        "                if len(l) > 0:\r\n",
        "                    l = l.strip('\\n').split(' ')\r\n",
        "                    items = [int(i) for i in l[1:]]\r\n",
        "                    # first element is the user_id, rest are items\r\n",
        "                    uid = int(l[0])\r\n",
        "                    self.exist_users.append(uid)\r\n",
        "                    # item/user with highest number is number of items/users\r\n",
        "                    self.n_items = max(self.n_items, max(items))\r\n",
        "                    self.n_users = max(self.n_users, uid)\r\n",
        "                    # number of interactions\r\n",
        "                    self.n_train += len(items)\r\n",
        "\r\n",
        "        # search test_file for max item_id\r\n",
        "        with open(test_file) as f:\r\n",
        "            for l in f.readlines():\r\n",
        "                if len(l) > 0:\r\n",
        "                    l = l.strip('\\n')\r\n",
        "                    try:\r\n",
        "                        items = [int(i) for i in l.split(' ')[1:]]\r\n",
        "                    except Exception:\r\n",
        "                        continue\r\n",
        "                    if not items:\r\n",
        "                        print(\"empyt test exists\")\r\n",
        "                        pass\r\n",
        "                    else:\r\n",
        "                        self.n_items = max(self.n_items, max(items))\r\n",
        "                        self.n_test += len(items)\r\n",
        "        # adjust counters: user_id/item_id starts at 0\r\n",
        "        self.n_items += 1\r\n",
        "        self.n_users += 1\r\n",
        "\r\n",
        "        self.print_statistics()\r\n",
        "\r\n",
        "        # create interactions/ratings matrix 'R' # dok = dictionary of keys\r\n",
        "        print('Creating interaction matrices R_train and R_test...')\r\n",
        "        t1 = time()\r\n",
        "        self.R_train = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32) \r\n",
        "        self.R_test = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\r\n",
        "\r\n",
        "        self.train_items, self.test_set = {}, {}\r\n",
        "        with open(train_file) as f_train:\r\n",
        "            with open(test_file) as f_test:\r\n",
        "                for l in f_train.readlines():\r\n",
        "                    if len(l) == 0: break\r\n",
        "                    l = l.strip('\\n')\r\n",
        "                    items = [int(i) for i in l.split(' ')]\r\n",
        "                    uid, train_items = items[0], items[1:]\r\n",
        "                    # enter 1 if user interacted with item\r\n",
        "                    for i in train_items:\r\n",
        "                        self.R_train[uid, i] = 1.\r\n",
        "                    self.train_items[uid] = train_items\r\n",
        "\r\n",
        "                for l in f_test.readlines():\r\n",
        "                    if len(l) == 0: break\r\n",
        "                    l = l.strip('\\n')\r\n",
        "                    try:\r\n",
        "                        items = [int(i) for i in l.split(' ')]\r\n",
        "                    except Exception:\r\n",
        "                        continue\r\n",
        "                    uid, test_items = items[0], items[1:]\r\n",
        "                    for i in test_items:\r\n",
        "                        self.R_test[uid, i] = 1.0\r\n",
        "                    self.test_set[uid] = test_items\r\n",
        "        print('Complete. Interaction matrices R_train and R_test created in', time() - t1, 'sec')\r\n",
        "\r\n",
        "    # if exist, get adjacency matrix\r\n",
        "    def get_adj_mat(self):\r\n",
        "        try:\r\n",
        "            t1 = time()\r\n",
        "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\r\n",
        "            print('Loaded adjacency-matrix (shape:', adj_mat.shape,') in', time() - t1, 'sec.')\r\n",
        "\r\n",
        "        except Exception:\r\n",
        "            print('Creating adjacency-matrix...')\r\n",
        "            adj_mat = self.create_adj_mat()\r\n",
        "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\r\n",
        "        return adj_mat\r\n",
        "    \r\n",
        "    # create adjancency matrix\r\n",
        "    def create_adj_mat(self):\r\n",
        "        t1 = time()\r\n",
        "        \r\n",
        "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\r\n",
        "        adj_mat = adj_mat.tolil()\r\n",
        "        R = self.R_train.tolil() # to list of lists\r\n",
        "\r\n",
        "        adj_mat[:self.n_users, self.n_users:] = R\r\n",
        "        adj_mat[self.n_users:, :self.n_users] = R.T\r\n",
        "        adj_mat = adj_mat.todok()\r\n",
        "        print('Complete. Adjacency-matrix created in', adj_mat.shape, time() - t1, 'sec.')\r\n",
        "\r\n",
        "        t2 = time()\r\n",
        "\r\n",
        "        # normalize adjacency matrix\r\n",
        "        def normalized_adj_single(adj):\r\n",
        "            rowsum = np.array(adj.sum(1))\r\n",
        "\r\n",
        "            d_inv = np.power(rowsum, -.5).flatten()\r\n",
        "            d_inv[np.isinf(d_inv)] = 0.\r\n",
        "            d_mat_inv = sp.diags(d_inv)\r\n",
        "\r\n",
        "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\r\n",
        "            return norm_adj.tocoo()\r\n",
        "\r\n",
        "        print('Transforming adjacency-matrix to NGCF-adjacency matrix...')\r\n",
        "        ngcf_adj_mat = normalized_adj_single(adj_mat) + sp.eye(adj_mat.shape[0])\r\n",
        "\r\n",
        "        print('Complete. Transformed adjacency-matrix to NGCF-adjacency matrix in', time() - t2, 'sec.')\r\n",
        "        return ngcf_adj_mat.tocsr()\r\n",
        "\r\n",
        "    # create collections of N items that users never interacted with\r\n",
        "    def negative_pool(self):\r\n",
        "        t1 = time()\r\n",
        "        for u in self.train_items.keys():\r\n",
        "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\r\n",
        "            pools = [rd.choice(neg_items) for _ in range(100)]\r\n",
        "            self.neg_pools[u] = pools\r\n",
        "        print('refresh negative pools', time() - t1)\r\n",
        "\r\n",
        "    # sample data for mini-batches\r\n",
        "    def sample(self):\r\n",
        "        if self.batch_size <= self.n_users:\r\n",
        "            users = rd.sample(self.exist_users, self.batch_size)\r\n",
        "        else:\r\n",
        "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\r\n",
        "\r\n",
        "        def sample_pos_items_for_u(u, num):\r\n",
        "            pos_items = self.train_items[u]\r\n",
        "            n_pos_items = len(pos_items)\r\n",
        "            pos_batch = []\r\n",
        "            while True:\r\n",
        "                if len(pos_batch) == num: break\r\n",
        "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\r\n",
        "                pos_i_id = pos_items[pos_id]\r\n",
        "\r\n",
        "                if pos_i_id not in pos_batch:\r\n",
        "                    pos_batch.append(pos_i_id)\r\n",
        "            return pos_batch\r\n",
        "\r\n",
        "        def sample_neg_items_for_u(u, num):\r\n",
        "            neg_items = []\r\n",
        "            while True:\r\n",
        "                if len(neg_items) == num: break\r\n",
        "                neg_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\r\n",
        "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\r\n",
        "                    neg_items.append(neg_id)\r\n",
        "            return neg_items\r\n",
        "\r\n",
        "        def sample_neg_items_for_u_from_pools(u, num):\r\n",
        "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\r\n",
        "            return rd.sample(neg_items, num)\r\n",
        "\r\n",
        "        pos_items, neg_items = [], []\r\n",
        "        for u in users:\r\n",
        "            pos_items += sample_pos_items_for_u(u, 1)\r\n",
        "            neg_items += sample_neg_items_for_u(u, 1)\r\n",
        "\r\n",
        "        return users, pos_items, neg_items\r\n",
        "\r\n",
        "    def get_num_users_items(self):\r\n",
        "        return self.n_users, self.n_items\r\n",
        "\r\n",
        "    def print_statistics(self):\r\n",
        "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\r\n",
        "        print('n_interactions=%d' % (self.n_train + self.n_test))\r\n",
        "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqaY8s98WA61"
      },
      "source": [
        "### 2.2: Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIySgH1X3X3b"
      },
      "source": [
        "import numpy as np\r\n",
        "import scipy.sparse as sp\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "import scipy.sparse as sp\r\n",
        "\r\n",
        "from torch import nn\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "class NGCF(nn.Module):\r\n",
        "    def __init__(self, n_users, n_items, emb_dim, layers, reg, node_dropout, mess_dropout,\r\n",
        "        adj_mtx):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # initialize Class attributes\r\n",
        "        self.n_users = n_users\r\n",
        "        self.n_items = n_items\r\n",
        "        self.emb_dim = emb_dim\r\n",
        "        self.adj_mtx = adj_mtx\r\n",
        "        self.laplacian = adj_mtx - sp.eye(adj_mtx.shape[0])\r\n",
        "        self.reg = reg\r\n",
        "        self.layers = layers\r\n",
        "        self.n_layers = len(self.layers)\r\n",
        "        self.node_dropout = node_dropout\r\n",
        "        self.mess_dropout = mess_dropout\r\n",
        "\r\n",
        "        #self.u_g_embeddings = nn.Parameter(torch.empty(n_users, emb_dim+np.sum(self.layers)))\r\n",
        "        #self.i_g_embeddings = nn.Parameter(torch.empty(n_items, emb_dim+np.sum(self.layers)))\r\n",
        "\r\n",
        "        # Initialize weights\r\n",
        "        self.weight_dict = self._init_weights()\r\n",
        "        print(\"Weights initialized.\")\r\n",
        "\r\n",
        "        # Create Matrix 'A', PyTorch sparse tensor of SP adjacency_mtx\r\n",
        "        self.A = self._convert_sp_mat_to_sp_tensor(self.adj_mtx)\r\n",
        "        self.L = self._convert_sp_mat_to_sp_tensor(self.laplacian)\r\n",
        "\r\n",
        "    # initialize weights\r\n",
        "    def _init_weights(self):\r\n",
        "        print(\"Initializing weights...\")\r\n",
        "        weight_dict = nn.ParameterDict()\r\n",
        "\r\n",
        "        initializer = torch.nn.init.xavier_uniform_\r\n",
        "        \r\n",
        "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\r\n",
        "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\r\n",
        "\r\n",
        "        weight_size_list = [self.emb_dim] + self.layers\r\n",
        "\r\n",
        "        for k in range(self.n_layers):\r\n",
        "            weight_dict['W_gc_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\r\n",
        "            weight_dict['b_gc_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\r\n",
        "            \r\n",
        "            weight_dict['W_bi_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\r\n",
        "            weight_dict['b_bi_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\r\n",
        "           \r\n",
        "        return weight_dict\r\n",
        "\r\n",
        "    # convert sparse matrix into sparse PyTorch tensor\r\n",
        "    def _convert_sp_mat_to_sp_tensor(self, X):\r\n",
        "        \"\"\"\r\n",
        "        Convert scipy sparse matrix to PyTorch sparse matrix\r\n",
        "\r\n",
        "        Arguments:\r\n",
        "        ----------\r\n",
        "        X = Adjacency matrix, scipy sparse matrix\r\n",
        "        \"\"\"\r\n",
        "        coo = X.tocoo().astype(np.float32)\r\n",
        "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\r\n",
        "        v = torch.FloatTensor(coo.data)\r\n",
        "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\r\n",
        "        return res\r\n",
        "\r\n",
        "    # apply node_dropout\r\n",
        "    def _droupout_sparse(self, X):\r\n",
        "        \"\"\"\r\n",
        "        Drop individual locations in X\r\n",
        "        \r\n",
        "        Arguments:\r\n",
        "        ---------\r\n",
        "        X = adjacency matrix (PyTorch sparse tensor)\r\n",
        "        dropout = fraction of nodes to drop\r\n",
        "        noise_shape = number of non non-zero entries of X\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\r\n",
        "        i = X.coalesce().indices()\r\n",
        "        v = X.coalesce()._values()\r\n",
        "        i[:,node_dropout_mask] = 0\r\n",
        "        v[node_dropout_mask] = 0\r\n",
        "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\r\n",
        "\r\n",
        "        return  X_dropout.mul(1/(1-self.node_dropout))\r\n",
        "\r\n",
        "    def forward(self, u, i, j):\r\n",
        "        \"\"\"\r\n",
        "        Computes the forward pass\r\n",
        "        \r\n",
        "        Arguments:\r\n",
        "        ---------\r\n",
        "        u = user\r\n",
        "        i = positive item (user interacted with item)\r\n",
        "        j = negative item (user did not interact with item)\r\n",
        "        \"\"\"\r\n",
        "        # apply drop-out mask\r\n",
        "        A_hat = self._droupout_sparse(self.A) if self.node_dropout > 0 else self.A\r\n",
        "        L_hat = self._droupout_sparse(self.L) if self.node_dropout > 0 else self.L\r\n",
        "\r\n",
        "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\r\n",
        "\r\n",
        "        all_embeddings = [ego_embeddings]\r\n",
        "\r\n",
        "        # forward pass for 'n' propagation layers\r\n",
        "        for k in range(self.n_layers):\r\n",
        "\r\n",
        "            # weighted sum messages of neighbours\r\n",
        "            side_embeddings = torch.sparse.mm(A_hat, ego_embeddings)\r\n",
        "            side_L_embeddings = torch.sparse.mm(L_hat, ego_embeddings)\r\n",
        "\r\n",
        "            # transformed sum weighted sum messages of neighbours\r\n",
        "            sum_embeddings = torch.matmul(side_embeddings, self.weight_dict['W_gc_%d' % k]) + self.weight_dict['b_gc_%d' % k]\r\n",
        "\r\n",
        "            # bi messages of neighbours\r\n",
        "            bi_embeddings = torch.mul(ego_embeddings, side_L_embeddings)\r\n",
        "            # transformed bi messages of neighbours\r\n",
        "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_bi_%d' % k]) + self.weight_dict['b_bi_%d' % k]\r\n",
        "\r\n",
        "            # non-linear activation \r\n",
        "            ego_embeddings = F.leaky_relu(sum_embeddings + bi_embeddings)\r\n",
        "            # + message dropout\r\n",
        "            mess_dropout_mask = nn.Dropout(self.mess_dropout)\r\n",
        "            ego_embeddings = mess_dropout_mask(ego_embeddings)\r\n",
        "\r\n",
        "            # normalize activation\r\n",
        "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\r\n",
        "\r\n",
        "            all_embeddings.append(norm_embeddings)\r\n",
        "\r\n",
        "        all_embeddings = torch.cat(all_embeddings, 1)\r\n",
        "        \r\n",
        "        # back to user/item dimension\r\n",
        "        u_g_embeddings, i_g_embeddings = all_embeddings.split([self.n_users, self.n_items], 0)\r\n",
        "\r\n",
        "        self.u_g_embeddings = nn.Parameter(u_g_embeddings)\r\n",
        "        self.i_g_embeddings = nn.Parameter(i_g_embeddings)\r\n",
        "        \r\n",
        "        u_emb = u_g_embeddings[u] # user embeddings\r\n",
        "        p_emb = i_g_embeddings[i] # positive item embeddings\r\n",
        "        n_emb = i_g_embeddings[j] # negative item embeddings\r\n",
        "\r\n",
        "        y_ui = torch.mul(u_emb, p_emb).sum(dim=1)\r\n",
        "        y_uj = torch.mul(u_emb, n_emb).sum(dim=1)\r\n",
        "        log_prob = (torch.log(torch.sigmoid(y_ui-y_uj))).mean()\r\n",
        "\r\n",
        "        # compute bpr-loss\r\n",
        "        bpr_loss = -log_prob\r\n",
        "        if self.reg > 0.:\r\n",
        "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\r\n",
        "            l2reg  = self.reg*l2norm\r\n",
        "            bpr_loss =  -log_prob + l2reg\r\n",
        "\r\n",
        "        return bpr_loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVqgf4AFXOGJ"
      },
      "source": [
        "### 2.3: Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG70NPZTU8-L"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "\r\n",
        "def early_stopping(log_value, best_value, stopping_step, flag_step, expected_order='asc'):\r\n",
        "    \"\"\"\r\n",
        "    Check if early_stopping is needed\r\n",
        "    Function copied from original code\r\n",
        "    \"\"\"\r\n",
        "    assert expected_order in ['asc', 'des']\r\n",
        "    if (expected_order == 'asc' and log_value >= best_value) or (expected_order == 'des' and log_value <= best_value):\r\n",
        "        stopping_step = 0\r\n",
        "        best_value = log_value\r\n",
        "    else:\r\n",
        "        stopping_step += 1\r\n",
        "\r\n",
        "    if stopping_step >= flag_step:\r\n",
        "        print(\"Early stopping at step: {} log:{}\".format(flag_step, log_value))\r\n",
        "        should_stop = True\r\n",
        "    else:\r\n",
        "        should_stop = False\r\n",
        "\r\n",
        "    return best_value, stopping_step, should_stop\r\n",
        "\r\n",
        "def train(model, data_generator, optimizer):\r\n",
        "    \"\"\"\r\n",
        "    Train the model PyTorch style\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "    ---------\r\n",
        "    model: PyTorch model\r\n",
        "    data_generator: Data object\r\n",
        "    optimizer: PyTorch optimizer\r\n",
        "    \"\"\"\r\n",
        "    model.train()\r\n",
        "    n_batch = data_generator.n_train // data_generator.batch_size + 1\r\n",
        "    running_loss=0\r\n",
        "    for _ in range(n_batch):\r\n",
        "        u, i, j = data_generator.sample()\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = model(u,i,j)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        running_loss += loss.item()\r\n",
        "    return running_loss\r\n",
        "\r\n",
        "def split_matrix(X, n_splits=100):\r\n",
        "    \"\"\"\r\n",
        "    Split a matrix/Tensor into n_folds (for the user embeddings and the R matrices)\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "    ---------\r\n",
        "    X: matrix to be split\r\n",
        "    n_folds: number of folds\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    -------\r\n",
        "    splits: split matrices\r\n",
        "    \"\"\"\r\n",
        "    splits = []\r\n",
        "    chunk_size = X.shape[0] // n_splits\r\n",
        "    for i in range(n_splits):\r\n",
        "        start = i * chunk_size\r\n",
        "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\r\n",
        "        splits.append(X[start:end])\r\n",
        "    return splits\r\n",
        "\r\n",
        "def compute_ndcg_k(pred_items, test_items, test_indices, k):\r\n",
        "    \"\"\"\r\n",
        "    Compute NDCG@k\r\n",
        "    \r\n",
        "    Arguments:\r\n",
        "    ---------\r\n",
        "    pred_items: binary tensor with 1s in those locations corresponding to the predicted item interactions\r\n",
        "    test_items: binary tensor with 1s in locations corresponding to the real test interactions\r\n",
        "    test_indices: tensor with the location of the top-k predicted items\r\n",
        "    k: k'th-order \r\n",
        "\r\n",
        "    Returns:\r\n",
        "    -------\r\n",
        "    NDCG@k\r\n",
        "    \"\"\"\r\n",
        "    r = (test_items * pred_items).gather(1, test_indices)\r\n",
        "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().cuda()\r\n",
        "    dcg = (r[:, :k]/f).sum(1)\r\n",
        "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)\r\n",
        "    ndcg = dcg/dcg_max\r\n",
        "    ndcg[torch.isnan(ndcg)] = 0\r\n",
        "    return ndcg\r\n",
        "\r\n",
        "\r\n",
        "def eval_model(u_emb, i_emb, Rtr, Rte, k):\r\n",
        "    \"\"\"\r\n",
        "    Evaluate the model\r\n",
        "    \r\n",
        "    Arguments:\r\n",
        "    ---------\r\n",
        "    u_emb: User embeddings\r\n",
        "    i_emb: Item embeddings\r\n",
        "    Rtr: Sparse matrix with the training interactions\r\n",
        "    Rte: Sparse matrix with the testing interactions\r\n",
        "    k : kth-order for metrics\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "    --------\r\n",
        "    result: Dictionary with lists correponding to the metrics at order k for k in Ks\r\n",
        "    \"\"\"\r\n",
        "    # split matrices\r\n",
        "    ue_splits = split_matrix(u_emb)\r\n",
        "    tr_splits = split_matrix(Rtr)\r\n",
        "    te_splits = split_matrix(Rte)\r\n",
        "\r\n",
        "    recall_k, ndcg_k= [], []\r\n",
        "    # compute results for split matrices\r\n",
        "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\r\n",
        "\r\n",
        "        scores = torch.mm(ue_f, i_emb.t())\r\n",
        "\r\n",
        "        test_items = torch.from_numpy(te_f.todense()).float().cuda()\r\n",
        "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().cuda()\r\n",
        "        scores = scores * non_train_items\r\n",
        "\r\n",
        "        _, test_indices = torch.topk(scores, dim=1, k=k)\r\n",
        "        pred_items = torch.zeros_like(scores).float()\r\n",
        "        pred_items.scatter_(dim=1,index=test_indices,value=torch.tensor(1.0).cuda())\r\n",
        "\r\n",
        "        topk_preds = torch.zeros_like(scores).float()\r\n",
        "        topk_preds.scatter_(dim=1,index=test_indices[:, :k],value=torch.tensor(1.0))\r\n",
        "\r\n",
        "        TP = (test_items * topk_preds).sum(1)\r\n",
        "        rec = TP/test_items.sum(1)\r\n",
        "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\r\n",
        "\r\n",
        "        recall_k.append(rec)\r\n",
        "        ndcg_k.append(ndcg)\r\n",
        "\r\n",
        "    return torch.cat(recall_k).mean(), torch.cat(ndcg_k).mean()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFtItdf7XUKK"
      },
      "source": [
        "### 2.4: Parameter and Base Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3uJEy123W5q"
      },
      "source": [
        "# Parameters Inicialization\r\n",
        "\r\n",
        "data_dir = source_path+'/data/'\r\n",
        "dataset = 'ml-100k'\r\n",
        "batch_size = 1024\r\n",
        "layers = eval('[64,64]')\r\n",
        "emb_dim = 64\r\n",
        "lr = 0.0001\r\n",
        "reg = 1e-5\r\n",
        "mess_dropout = 0.1\r\n",
        "node_dropout = 0.\r\n",
        "k = 10\r\n",
        "n_epochs = 400\r\n",
        "eval_N = 1\r\n",
        "save_results = 1\r\n",
        "results_dir = 'results'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNHrZVLvg7hU",
        "outputId": "5a3417e5-e3bf-4329-f027-d949d908f685"
      },
      "source": [
        "data_generator = Data(path=data_dir + dataset, batch_size=batch_size)\r\n",
        "adj_mtx = data_generator.get_adj_mat()\r\n",
        "\r\n",
        "model = NGCF(data_generator.n_users, \r\n",
        "              data_generator.n_items,\r\n",
        "              emb_dim,\r\n",
        "              layers,\r\n",
        "              reg,\r\n",
        "              node_dropout,\r\n",
        "              mess_dropout,\r\n",
        "              adj_mtx)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_users=943, n_items=1682\n",
            "n_interactions=100000\n",
            "n_train=80064, n_test=19936, sparsity=0.06305\n",
            "Creating interaction matrices R_train and R_test...\n",
            "Complete. Interaction matrices R_train and R_test created in 1.0580151081085205 sec\n",
            "Loaded adjacency-matrix (shape: (2625, 2625) ) in 0.01728987693786621 sec.\n",
            "Initializing weights...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Weights initialized.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPqvvxxsXaou"
      },
      "source": [
        "### 2.5: Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DU_OqpQU0bV",
        "outputId": "dcef28c9-e7de-4d20-d26a-d73e657baef6"
      },
      "source": [
        "if use_cuda:\r\n",
        "    model = model.cuda()\r\n",
        "\r\n",
        "# current best metric\r\n",
        "cur_best_metric = 0\r\n",
        "\r\n",
        "# Adam optimizer\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "# Set values for early stopping\r\n",
        "cur_best_loss, stopping_step, should_stop = 1e3, 0, False\r\n",
        "today = datetime.now()\r\n",
        "\r\n",
        "print(\"Start at \" + str(today))\r\n",
        "print(\"Using \" + str(device) + \" for computations\")\r\n",
        "print(\"Params on CUDA: \" + str(next(model.parameters()).is_cuda))\r\n",
        "\r\n",
        "results = {\"Epoch\": [],\r\n",
        "            \"Loss\": [],\r\n",
        "            \"Recall\": [],\r\n",
        "            \"NDCG\": [],\r\n",
        "            \"Training Time\": []}\r\n",
        "\r\n",
        "for epoch in range(n_epochs):\r\n",
        "\r\n",
        "    t1 = time()\r\n",
        "    loss = train(model, data_generator, optimizer)\r\n",
        "    training_time = time()-t1\r\n",
        "    print(\"Epoch: {}, Training time: {:.2f}s, Loss: {:.4f}\".\r\n",
        "        format(epoch, training_time, loss))\r\n",
        "\r\n",
        "    # print test evaluation metrics every N epochs (provided by eval_N)\r\n",
        "    if epoch % eval_N  == (eval_N - 1):\r\n",
        "        with torch.no_grad():\r\n",
        "            t2 = time()\r\n",
        "            recall, ndcg = eval_model(model.u_g_embeddings.detach(),\r\n",
        "                                      model.i_g_embeddings.detach(),\r\n",
        "                                      data_generator.R_train,\r\n",
        "                                      data_generator.R_test,\r\n",
        "                                      k)\r\n",
        "        print(\r\n",
        "            \"Evaluate current model:\\n\",\r\n",
        "            \"Epoch: {}, Validation time: {:.2f}s\".format(epoch, time()-t2),\"\\n\",\r\n",
        "            \"Loss: {:.4f}:\".format(loss), \"\\n\",\r\n",
        "            \"Recall@{}: {:.4f}\".format(k, recall), \"\\n\",\r\n",
        "            \"NDCG@{}: {:.4f}\".format(k, ndcg)\r\n",
        "            )\r\n",
        "\r\n",
        "        cur_best_metric, stopping_step, should_stop = \\\r\n",
        "        early_stopping(recall, cur_best_metric, stopping_step, flag_step=5)\r\n",
        "\r\n",
        "        # save results in dict\r\n",
        "        results['Epoch'].append(epoch)\r\n",
        "        results['Loss'].append(loss)\r\n",
        "        results['Recall'].append(recall.item())\r\n",
        "        results['NDCG'].append(ndcg.item())\r\n",
        "        results['Training Time'].append(training_time)\r\n",
        "    else:\r\n",
        "        # save results in dict\r\n",
        "        results['Epoch'].append(epoch)\r\n",
        "        results['Loss'].append(loss)\r\n",
        "        results['Recall'].append(None)\r\n",
        "        results['NDCG'].append(None)\r\n",
        "        results['Training Time'].append(training_time)\r\n",
        "\r\n",
        "    if should_stop == True: break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start at 2021-01-30 22:18:15.919897\n",
            "Using cuda for computations\n",
            "Params on CUDA: True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Training time: 3.97s, Loss: 54.4709\n",
            "Evaluate current model:\n",
            " Epoch: 0, Validation time: 1.12s \n",
            " Loss: 54.4709: \n",
            " Recall@10: 0.0102 \n",
            " NDCG@10: 0.1195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training time: 3.99s, Loss: 53.9444\n",
            "Evaluate current model:\n",
            " Epoch: 1, Validation time: 1.13s \n",
            " Loss: 53.9444: \n",
            " Recall@10: 0.0114 \n",
            " NDCG@10: 0.1164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2, Training time: 3.88s, Loss: 52.1963\n",
            "Evaluate current model:\n",
            " Epoch: 2, Validation time: 1.16s \n",
            " Loss: 52.1963: \n",
            " Recall@10: 0.0126 \n",
            " NDCG@10: 0.1400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3, Training time: 3.88s, Loss: 44.9912\n",
            "Evaluate current model:\n",
            " Epoch: 3, Validation time: 1.12s \n",
            " Loss: 44.9912: \n",
            " Recall@10: 0.0214 \n",
            " NDCG@10: 0.1629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4, Training time: 3.91s, Loss: 40.4744\n",
            "Evaluate current model:\n",
            " Epoch: 4, Validation time: 1.12s \n",
            " Loss: 40.4744: \n",
            " Recall@10: 0.0220 \n",
            " NDCG@10: 0.1558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5, Training time: 3.89s, Loss: 39.2227\n",
            "Evaluate current model:\n",
            " Epoch: 5, Validation time: 1.14s \n",
            " Loss: 39.2227: \n",
            " Recall@10: 0.0287 \n",
            " NDCG@10: 0.1829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 6, Training time: 3.90s, Loss: 38.0954\n",
            "Evaluate current model:\n",
            " Epoch: 6, Validation time: 1.12s \n",
            " Loss: 38.0954: \n",
            " Recall@10: 0.0327 \n",
            " NDCG@10: 0.1990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 7, Training time: 3.94s, Loss: 37.1410\n",
            "Evaluate current model:\n",
            " Epoch: 7, Validation time: 1.13s \n",
            " Loss: 37.1410: \n",
            " Recall@10: 0.0364 \n",
            " NDCG@10: 0.1997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 8, Training time: 3.99s, Loss: 35.9227\n",
            "Evaluate current model:\n",
            " Epoch: 8, Validation time: 1.13s \n",
            " Loss: 35.9227: \n",
            " Recall@10: 0.0405 \n",
            " NDCG@10: 0.2213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 9, Training time: 3.95s, Loss: 34.2204\n",
            "Evaluate current model:\n",
            " Epoch: 9, Validation time: 1.13s \n",
            " Loss: 34.2204: \n",
            " Recall@10: 0.0419 \n",
            " NDCG@10: 0.2254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, Training time: 3.85s, Loss: 33.0042\n",
            "Evaluate current model:\n",
            " Epoch: 10, Validation time: 1.13s \n",
            " Loss: 33.0042: \n",
            " Recall@10: 0.0473 \n",
            " NDCG@10: 0.2478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 11, Training time: 3.88s, Loss: 31.9640\n",
            "Evaluate current model:\n",
            " Epoch: 11, Validation time: 1.13s \n",
            " Loss: 31.9640: \n",
            " Recall@10: 0.0495 \n",
            " NDCG@10: 0.2499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 12, Training time: 3.92s, Loss: 30.9968\n",
            "Evaluate current model:\n",
            " Epoch: 12, Validation time: 1.12s \n",
            " Loss: 30.9968: \n",
            " Recall@10: 0.0548 \n",
            " NDCG@10: 0.2763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 13, Training time: 3.89s, Loss: 30.5121\n",
            "Evaluate current model:\n",
            " Epoch: 13, Validation time: 1.12s \n",
            " Loss: 30.5121: \n",
            " Recall@10: 0.0508 \n",
            " NDCG@10: 0.2623\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 14, Training time: 4.03s, Loss: 29.8741\n",
            "Evaluate current model:\n",
            " Epoch: 14, Validation time: 1.12s \n",
            " Loss: 29.8741: \n",
            " Recall@10: 0.0562 \n",
            " NDCG@10: 0.2798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 15, Training time: 3.90s, Loss: 29.4283\n",
            "Evaluate current model:\n",
            " Epoch: 15, Validation time: 1.13s \n",
            " Loss: 29.4283: \n",
            " Recall@10: 0.0754 \n",
            " NDCG@10: 0.3111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 16, Training time: 3.92s, Loss: 29.2288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eYk4IfslS96"
      },
      "source": [
        "### 2.6 Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgWFLo3kYB14"
      },
      "source": [
        "# save\r\n",
        "if save_results:\r\n",
        "    date = today.strftime(\"%d%m%Y_%H%M\")\r\n",
        "\r\n",
        "    # save model as .pt file\r\n",
        "    if os.path.isdir(\"./models\"):\r\n",
        "        torch.save(model.state_dict(), \"./models/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".pt\")\r\n",
        "    else:\r\n",
        "        os.mkdir(\"./models\")\r\n",
        "        torch.save(model.state_dict(), \"./models/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".pt\")\r\n",
        "\r\n",
        "    # save results as pandas dataframe\r\n",
        "    results_df = pd.DataFrame(results)\r\n",
        "    results_df.set_index('Epoch', inplace=True)\r\n",
        "    if os.path.isdir(\"./results\"):\r\n",
        "        results_df.to_csv(\"./results/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".csv\")\r\n",
        "    else:\r\n",
        "        os.mkdir(\"./results\")\r\n",
        "        results_df.to_csv(\"./results/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".csv\")\r\n",
        "    # plot loss\r\n",
        "    results_df['Loss'].plot(figsize=(12,8), title='Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hhl0V7hYBMP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}